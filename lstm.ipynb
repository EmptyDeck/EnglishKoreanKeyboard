{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Define the allowed characters (26 lowercase + 26 uppercase)\n",
    "ALPHABET = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "char_to_idx = {char: idx for idx, char in enumerate(ALPHABET)}\n",
    "\n",
    "def one_hot_encode(letter):\n",
    "    \"\"\"Return a 52-dim one-hot vector for a given letter.\"\"\"\n",
    "    vec = np.zeros(len(ALPHABET), dtype=np.float32)\n",
    "    if letter in char_to_idx:\n",
    "        vec[char_to_idx[letter]] = 1.0\n",
    "    return vec\n",
    "\n",
    "class WordDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.data = pd.read_csv(csv_file)  # CSV with columns: word, isEnlgish\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        word = str(row['word'])  # Convert to string in case it's a float\n",
    "        label = float(row['isEnlgish'])  # 1 for English, 0 for Korean\n",
    "        # Convert each character in the word to its one-hot representation\n",
    "        word_seq = [one_hot_encode(ch) for ch in word if ch in char_to_idx]\n",
    "        word_seq = torch.tensor(word_seq, dtype=torch.float32)  # shape: (seq_length, 52)\n",
    "        label = torch.tensor([label], dtype=torch.float32)  # shape: (1,)\n",
    "        return word_seq, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\torchenv\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\torchenv\\Lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\torchenv\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 711, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\torchenv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\torchenv\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\torchenv\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\torchenv\\Lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\torchenv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\torchenv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\torchenv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\torchenv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 729, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\torchenv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 411, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\torchenv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 531, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\torchenv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3006, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\torchenv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3061, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\torchenv\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\torchenv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3266, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\torchenv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3445, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\torchenv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\sejik\\AppData\\Local\\Temp\\ipykernel_37820\\1903287218.py\", line 1, in <module>\n",
      "    WordDataset('words.csv')[0]\n",
      "  File \"C:\\Users\\sejik\\AppData\\Local\\Temp\\ipykernel_37820\\1456752896.py\", line 33, in __getitem__\n",
      "    word_seq = torch.tensor(word_seq, dtype=torch.float32)  # shape: (seq_length, 52)\n",
      "C:\\Users\\sejik\\AppData\\Local\\Temp\\ipykernel_37820\\1456752896.py:33: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  word_seq = torch.tensor(word_seq, dtype=torch.float32)  # shape: (seq_length, 52)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([0.]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordDataset('words.csv')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    sequences, labels = zip(*batch)\n",
    "    # Determine lengths of each sequence\n",
    "    lengths = torch.tensor([seq.shape[0] for seq in sequences])\n",
    "    # Pad sequences so they all have the same length (batch_first=True gives shape: (batch, max_length, 52))\n",
    "    padded_seqs = pad_sequence(sequences, batch_first=True)\n",
    "    labels = torch.stack(labels)  # shape: (batch, 1)\n",
    "    return padded_seqs, labels, lengths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageClassifier(nn.Module):\n",
    "    def __init__(self, input_size=52, hidden_size=128, num_layers=1):\n",
    "        super(LanguageClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, lengths):\n",
    "        # Pack the padded sequence\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_out, (h_n, _) = self.lstm(packed)\n",
    "        # h_n is of shape (num_layers, batch, hidden_size); we use the last layerâ€™s hidden state\n",
    "        h_last = h_n[-1]  # shape: (batch, hidden_size)\n",
    "        out = self.fc(h_last)  # shape: (batch, 1)\n",
    "        out = self.sigmoid(out)  # Output between 0 and 1\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and DataLoader\n",
    "dataset = WordDataset(\"words.csv\") # Replace 'words.csv' with your dataset path\n",
    "# Calculate dataset sizes\n",
    "total_size = len(dataset)\n",
    "train_size = int(0.7 * total_size)\n",
    "remaining_size = total_size - train_size\n",
    "val_size = int(remaining_size / 2)\n",
    "test_size = remaining_size - val_size\n",
    "\n",
    "\n",
    "# Use random_split to create training, validation, and test datasets\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn) # No need to shuffle val/test\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn) # No need to shuffle val/test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model, Criterion, Optimizer ---\n",
    "model = LanguageClassifier(input_size=52, hidden_size=128, num_layers=1)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Bringing It All Together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.3398 Validation Loss: 0.2054\n",
      "Epoch 2, Training Loss: 0.1840 Validation Loss: 0.1721\n",
      "Epoch 3, Training Loss: 0.1529 Validation Loss: 0.1712\n",
      "Epoch 4, Training Loss: 0.1295 Validation Loss: 0.1421\n",
      "Epoch 5, Training Loss: 0.1140 Validation Loss: 0.1234\n",
      "Epoch 6, Training Loss: 0.1050 Validation Loss: 0.1162\n",
      "Epoch 7, Training Loss: 0.0860 Validation Loss: 0.0948\n",
      "Epoch 8, Training Loss: 0.0715 Validation Loss: 0.0865\n",
      "Epoch 9, Training Loss: 0.0601 Validation Loss: 0.0721\n",
      "Epoch 10, Training Loss: 0.0518 Validation Loss: 0.0584\n",
      "Epoch 11, Training Loss: 0.0441 Validation Loss: 0.0518\n",
      "Epoch 12, Training Loss: 0.0395 Validation Loss: 0.0473\n",
      "Epoch 13, Training Loss: 0.0385 Validation Loss: 0.0575\n",
      "Epoch 14, Training Loss: 0.0328 Validation Loss: 0.0506\n",
      "Epoch 15, Training Loss: 0.0280 Validation Loss: 0.0472\n",
      "Epoch 16, Training Loss: 0.0287 Validation Loss: 0.0481\n",
      "Epoch 17, Training Loss: 0.0249 Validation Loss: 0.0517\n",
      "Epoch 18, Training Loss: 0.0230 Validation Loss: 0.0450\n",
      "Epoch 19, Training Loss: 0.0219 Validation Loss: 0.0436\n",
      "Epoch 20, Training Loss: 0.0213 Validation Loss: 0.0453\n",
      "Test Loss: 0.0577\n"
     ]
    }
   ],
   "source": [
    "# --- Training and Validation loop ---\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    for batch in train_dataloader: # Use train_dataloader\n",
    "        inputs, labels, lengths = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, lengths)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss/len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {avg_train_loss:.4f}\", end=\" \")\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval() # Set model to evaluation mode for validation\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad(): # Disable gradient calculation for validation\n",
    "        for batch in val_dataloader: # Use val_dataloader\n",
    "            inputs, labels, lengths = batch\n",
    "            outputs = model(inputs, lengths)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "    avg_val_loss = val_loss/len(val_dataloader)\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "    model.train() # Set model back to training mode\n",
    "\n",
    "\n",
    "# --- Evaluation on Test set ---\n",
    "model.eval() # Set model to evaluation mode for testing\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader: # Use test_dataloader\n",
    "        inputs, labels, lengths = batch\n",
    "        outputs = model(inputs, lengths)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "avg_test_loss = test_loss/len(test_dataloader)\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for 'hello': 0.9997\n"
     ]
    }
   ],
   "source": [
    "# Example of real-time prediction for a new word\n",
    "model.eval()\n",
    "test_word = \"hello\"\n",
    "test_seq = [one_hot_encode(ch) for ch in test_word if ch in char_to_idx]\n",
    "test_seq = torch.tensor(test_seq, dtype=torch.float32).unsqueeze(0)  # shape: (1, seq_length, 52)\n",
    "length = torch.tensor([len(test_seq[0])])\n",
    "with torch.no_grad():\n",
    "    prediction = model(test_seq, length)\n",
    "print(f\"Prediction for '{test_word}': {prediction.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
